{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e1b8464-252d-4b42-a735-404740cc6f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 50/50 [05:19<00:00,  6.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE! FULL: (43545978, 20) | DAILY: (997883, 13)\n"
     ]
    }
   ],
   "source": [
    "# The file generated by this code is currently used to prepare the “Rider Behavior Overview.”\n",
    "\n",
    "import os, pandas as pd, re, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_DIR = \"/Users/saurabh/Documents/New Haven/Curriculam/GA/Citi Bikes/datasets/citibike_2025_raw\"\n",
    "COL_ORDER = [\"ride_id\",\"rideable_type\",\"started_at\",\"ended_at\",\"start_station_name\",\"start_station_id\",\"end_station_name\",\"end_station_id\",\"start_lat\",\"start_lng\",\"end_lat\",\"end_lng\",\"member_casual\"]\n",
    "\n",
    "all_csv_paths = [os.path.join(root,f) for root,dirs,files in os.walk(BASE_DIR) for f in files if f.endswith('.csv')]\n",
    "print(f\"Found {len(all_csv_paths)} files\")\n",
    "\n",
    "all_dfs = []\n",
    "for full_path in tqdm(all_csv_paths):\n",
    "    filename = os.path.basename(full_path)\n",
    "    df = pd.read_csv(full_path, low_memory=False)\n",
    "    df = df.reindex(columns=COL_ORDER)\n",
    "    \n",
    "    df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "    df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n",
    "    df['month_year'] = df['started_at'].dt.strftime('%Y-%m')\n",
    "    df['month_name'] = df['started_at'].dt.strftime('%B %Y')\n",
    "    df['year'] = df['started_at'].dt.year\n",
    "    df['month'] = df['started_at'].dt.month\n",
    "    \n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Combine + analyze\n",
    "combined = pd.concat(all_dfs, ignore_index=True)\n",
    "combined = combined.dropna(subset=['started_at','ended_at','start_station_name','end_station_name'])\n",
    "combined['hour_started_at'] = combined['started_at'].dt.hour\n",
    "combined['ride_duration_sec'] = (combined['ended_at']-combined['started_at']).dt.total_seconds()\n",
    "\n",
    "combined = combined[combined['ride_duration_sec']>0]\n",
    "\n",
    "# Calculate distance\n",
    "with np.errstate(invalid='ignore'):\n",
    "    combined['distance_miles'] = 3959 * np.arccos(\n",
    "        np.sin(np.radians(combined['start_lat'])) * np.sin(np.radians(combined['end_lat'])) +\n",
    "        np.cos(np.radians(combined['start_lat'])) * np.cos(np.radians(combined['end_lat'])) * \n",
    "        np.cos(np.radians(combined['start_lng']) - np.radians(combined['end_lng']))\n",
    "    )\n",
    "    \n",
    "\n",
    "daily_hourly = combined.groupby(['month_year','month_name','hour_started_at','start_station_name','member_casual']).agg({\n",
    "    'ride_id':'count',\n",
    "    'ride_duration_sec':['sum','mean','median'],\n",
    "    'distance_miles': 'sum'\n",
    "}).reset_index().droplevel(axis=1,level=0)\n",
    "\n",
    "daily_hourly.columns = ['month_year','month_name','hour_started_at','start_station_name','member_casual',\n",
    "                       'trips','total_ride_duration_sec','avg_ride_duration_sec','median_ride_duration_sec',\n",
    "                       'total_distance_miles']\n",
    "\n",
    "for col in ['avg_ride_duration_min','median_ride_duration_min','total_ride_duration_min']:\n",
    "    daily_hourly[col] = daily_hourly[col.replace('_min','_sec')] / 60\n",
    "\n",
    "\n",
    "# Save\n",
    "combined.to_csv(\"citibike_2025_cleaned_full.csv\", index=False)\n",
    "daily_hourly.to_csv(\"citibike_2025_daily_hourly_stations.csv\", index=False)\n",
    "\n",
    "print(f\"DONE! FULL: {combined.shape} | DAILY: {daily_hourly.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f654f788-30f6-4046-9c79-3dad3c099862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging 202501 files...\n",
      "Found 3 202501 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging:   0%|                                            | 0/3 [00:00<?, ?it/s]/var/folders/rp/mkwcs2k94r570kpc9y6hsywc0000gn/T/ipykernel_35595/1584646144.py:27: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "Merging:  67%|████████████████████████            | 2/3 [00:02<00:00,  1.13it/s]/var/folders/rp/mkwcs2k94r570kpc9y6hsywc0000gn/T/ipykernel_35595/1584646144.py:27: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "Merging: 100%|████████████████████████████████████| 3/3 [00:03<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging dataframes...\n",
      "Raw merged: 2,124,475 rows\n",
      "After filters: 180 rows\n",
      "Hours range: 12-12\n",
      "✅ Saved: 202501_merged_filtered.csv\n",
      "\n",
      "Hour distribution:\n",
      "hour_started_at\n",
      "12    180\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample data:\n",
      "                   started_at                ended_at  hour_started_at  \\\n",
      "268   2025-01-07 12:42:07.719 2025-01-07 12:44:55.044               12   \n",
      "6661  2025-01-12 12:36:41.926 2025-01-12 12:59:55.533               12   \n",
      "11827 2025-01-09 12:55:29.886 2025-01-09 13:03:36.787               12   \n",
      "24017 2025-01-08 12:25:36.259 2025-01-08 12:41:43.534               12   \n",
      "25222 2025-01-10 12:26:45.532 2025-01-10 12:29:04.530               12   \n",
      "\n",
      "      start_station_name  \n",
      "268    S 2 St & Kent Ave  \n",
      "6661   S 2 St & Kent Ave  \n",
      "11827  S 2 St & Kent Ave  \n",
      "24017  S 2 St & Kent Ave  \n",
      "25222  S 2 St & Kent Ave  \n"
     ]
    }
   ],
   "source": [
    "# This code is used to validate aggregated data by performing random checks for selected months, hours, and stations.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CONFIG - CHANGE HERE ONLY\n",
    "BASE_DIR = \"/Users/saurabh/Documents/New Haven/Curriculam/GA/Citi Bikes/datasets/citibike_2025_raw\"\n",
    "MONTH_TARGET = \"202501\"  # January 2025\n",
    "HOUR_FILTER = [12]  # or [7,8,9]\n",
    "STATION_FILTER = [\"S 2 St & Kent Ave\"]  # or [\"W 20 St & 7 Ave\"]\n",
    "\n",
    "print(f\"Merging {MONTH_TARGET} files...\")\n",
    "\n",
    "# Find MONTH files only\n",
    "month_files = []\n",
    "for root, dirs, files in os.walk(BASE_DIR):\n",
    "    for file in files:\n",
    "        if file.lower().endswith('.csv') and re.search(rf'{MONTH_TARGET}', file):\n",
    "            month_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(month_files)} {MONTH_TARGET} files\")\n",
    "\n",
    "# Merge ALL month files\n",
    "all_dfs = []\n",
    "for file_path in tqdm(month_files, desc=\"Merging\"):\n",
    "    df = pd.read_csv(file_path)\n",
    "    all_dfs.append(df)\n",
    "\n",
    "print(\"Merging dataframes...\")\n",
    "merged = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"Raw merged: {len(merged):,} rows\")\n",
    "\n",
    "# Convert datetime ONLY (no other changes)\n",
    "merged['started_at'] = pd.to_datetime(merged['started_at'], errors='coerce')\n",
    "merged['ended_at'] = pd.to_datetime(merged['ended_at'], errors='coerce')\n",
    "\n",
    "# Add hour column ONLY\n",
    "merged['hour_started_at'] = merged['started_at'].dt.hour\n",
    "\n",
    "# Filter (optional)\n",
    "if HOUR_FILTER:\n",
    "    merged = merged[merged['hour_started_at'].isin(HOUR_FILTER)]\n",
    "if STATION_FILTER:\n",
    "    merged = merged[merged['start_station_name'].isin(STATION_FILTER)]\n",
    "\n",
    "print(f\"After filters: {len(merged):,} rows\")\n",
    "print(f\"Hours range: {merged['hour_started_at'].min()}-{merged['hour_started_at'].max()}\")\n",
    "\n",
    "# Save raw merged data\n",
    "merged.to_csv(f\"{MONTH_TARGET}_merged_filtered.csv\", index=False)\n",
    "print(f\"Saved: {MONTH_TARGET}_merged_filtered.csv\")\n",
    "\n",
    "# Quick check\n",
    "print(\"\\nHour distribution:\")\n",
    "print(merged['hour_started_at'].value_counts().sort_index().head(10))\n",
    "print(\"\\nSample data:\")\n",
    "print(merged[['started_at', 'ended_at', 'hour_started_at', 'start_station_name']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eae0d520-8691-4b72-acdc-f057b44f959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|██████████████████████████████████| 50/50 [14:00<00:00, 16.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating distances...\n",
      "Raw rides: 43,545,978\n",
      "Creating hourly station flows...\n",
      "\n",
      "SAVED: citibike_2025_station_flows_hourly.csv\n",
      "Columns: ['month_year', 'hour_started_at', 'start_station_name', 'end_station_name', 'member_casual', 'total_trips', 'total_trip_duration_min', 'total_distance_miles']\n"
     ]
    }
   ],
   "source": [
    "# This code creates a separate file for the station-level dashboard. It is currently not in use because the number of rows generated exceeds the BI tool’s capacity.\n",
    "\n",
    "import os, pandas as pd, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_DIR = \"/Users/saurabh/Documents/New Haven/Curriculam/GA/Citi Bikes/datasets/citibike_2025_raw\"\n",
    "COL_ORDER = [\"ride_id\",\"rideable_type\",\"started_at\",\"ended_at\",\"start_station_name\",\"start_station_id\",\"end_station_name\",\"end_station_id\",\"start_lat\",\"start_lng\",\"end_lat\",\"end_lng\",\"member_casual\"]\n",
    "\n",
    "# Load + clean\n",
    "all_csv_paths = [os.path.join(root,f) for root,dirs,files in os.walk(BASE_DIR) for f in files if f.endswith('.csv')]\n",
    "print(f\"Found {len(all_csv_paths)} files\")\n",
    "\n",
    "all_dfs = []\n",
    "for full_path in tqdm(all_csv_paths, desc=\"Loading\"):\n",
    "    df = pd.read_csv(full_path, low_memory=False)\n",
    "    df = df.reindex(columns=COL_ORDER)\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "    df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n",
    "    df['month_year'] = df['started_at'].dt.strftime('%Y-%m')   \n",
    "    df['hour_started_at'] = df['started_at'].dt.hour    \n",
    "    all_dfs.append(df)\n",
    "\n",
    "combined = pd.concat(all_dfs, ignore_index=True)\n",
    "combined = combined.dropna(subset=['started_at','ended_at','start_station_name','end_station_name'])\n",
    "combined['hour_started_at'] = combined['started_at'].dt.hour\n",
    "combined['ride_duration_sec'] = (combined['ended_at']-combined['started_at']).dt.total_seconds()\n",
    "combined = combined[combined['ride_duration_sec']>0]\n",
    "\n",
    "# Distance\n",
    "print(\"Calculating distances...\")\n",
    "with np.errstate(invalid='ignore'):\n",
    "    combined['distance_miles'] = 3959 * np.arccos(\n",
    "        np.sin(np.radians(combined['start_lat'])) * np.sin(np.radians(combined['end_lat'])) +\n",
    "        np.cos(np.radians(combined['start_lat'])) * np.cos(np.radians(combined['end_lat'])) * \n",
    "        np.cos(np.radians(combined['start_lng']) - np.radians(combined['end_lng']))\n",
    "    )\n",
    "\n",
    "print(f\"Raw rides: {combined.shape[0]:,}\")\n",
    "\n",
    "\n",
    "print(\"Creating hourly station flows...\")\n",
    "station_flows = combined.groupby(['month_year','hour_started_at','start_station_name','end_station_name','member_casual']).agg({\n",
    "    'ride_id':'count',           # total_trips\n",
    "    'ride_duration_sec':'sum',   # total_trip_duration_sec\n",
    "    'distance_miles':'sum'       # total_distance_miles\n",
    "}).reset_index()\n",
    "\n",
    "station_flows.columns = ['month_year','hour_started_at','start_station_name','end_station_name','member_casual',\n",
    "                        'total_trips','total_trip_duration_sec','total_distance_miles']\n",
    "\n",
    "# Convert to minutes\n",
    "station_flows['total_trip_duration_min'] = station_flows['total_trip_duration_sec'] / 60\n",
    "\n",
    "\n",
    "station_flows = station_flows[['month_year', 'hour_started_at', 'start_station_name', 'end_station_name', \n",
    "                              'member_casual', 'total_trips', 'total_trip_duration_min', \n",
    "                              'total_distance_miles']]\n",
    "\n",
    "\n",
    "# Save\n",
    "output_file = \"citibike_2025_station_flows_hourly.csv\"\n",
    "station_flows.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nSAVED: {output_file}\")\n",
    "print(f\"Columns: {list(station_flows.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c8b1d9d-f7b9-4686-b5b0-3a4d06662685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|██████████████████████████████████| 50/50 [06:34<00:00,  7.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating distances...\n",
      "Creating SLIM station flows...\n"
     ]
    }
   ],
   "source": [
    "# This improved code includes all required columns for both Rider and Station dashboards. \n",
    "# It is the preferred approach but is not currently used due to limitations of the free BI tool.\n",
    "\n",
    "import os, pandas as pd, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "BASE_DIR = \"/Users/saurabh/Documents/New Haven/Curriculam/GA/Citi Bikes/datasets/citibike_2025_raw\"\n",
    "COL_ORDER = [\"ride_id\",\"rideable_type\",\"started_at\",\"ended_at\",\"start_station_name\",\"start_station_id\",\"end_station_name\",\"end_station_id\",\"start_lat\",\"start_lng\",\"end_lat\",\"end_lng\",\"member_casual\"]\n",
    "\n",
    "# Load + clean\n",
    "all_csv_paths = [os.path.join(root,f) for root,dirs,files in os.walk(BASE_DIR) for f in files if f.endswith('.csv')]\n",
    "print(f\"Found {len(all_csv_paths)} files\")\n",
    "\n",
    "all_dfs = []\n",
    "for full_path in tqdm(all_csv_paths, desc=\"Loading\"):\n",
    "    df = pd.read_csv(full_path, low_memory=False)\n",
    "    df = df.reindex(columns=COL_ORDER)\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "    df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n",
    "    df['month_year'] = df['started_at'].dt.strftime('%Y-%m')\n",
    "    df['month_name'] = df['started_at'].dt.strftime('%B %Y')\n",
    "    df['year'] = df['started_at'].dt.year\n",
    "    df['month'] = df['started_at'].dt.month\n",
    "    all_dfs.append(df)\n",
    "\n",
    "# Combine + filter\n",
    "combined = pd.concat(all_dfs, ignore_index=True)\n",
    "combined = combined.dropna(subset=['started_at','ended_at','start_station_name','end_station_name'])\n",
    "combined['hour_started_at'] = combined['started_at'].dt.hour\n",
    "combined['ride_duration_sec'] = (combined['ended_at']-combined['started_at']).dt.total_seconds()\n",
    "combined = combined[combined['ride_duration_sec']>0]\n",
    "\n",
    "# Distance\n",
    "print(\"Calculating distances...\")\n",
    "with np.errstate(invalid='ignore'):\n",
    "    combined['distance_miles'] = 3959 * np.arccos(\n",
    "        np.sin(np.radians(combined['start_lat'])) * np.sin(np.radians(combined['end_lat'])) +\n",
    "        np.cos(np.radians(combined['start_lat'])) * np.cos(np.radians(combined['end_lat'])) * \n",
    "        np.cos(np.radians(combined['start_lng']) - np.radians(combined['end_lng']))\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Creating SLIM station flows...\")\n",
    "daily_hourly = combined.groupby(['month_year','hour_started_at','start_station_name','end_station_name','member_casual']).agg({\n",
    "    'ride_id':'count',           # trips\n",
    "    'ride_duration_sec':'sum',   # total_duration_sec\n",
    "    'distance_miles':'sum'       # total_distance_miles\n",
    "}).reset_index()\n",
    "\n",
    "daily_hourly.columns = ['month_year','hour_started_at','start_station_name','end_station_name','member_casual',\n",
    "                       'trips','total_ride_duration_sec','total_distance_miles']\n",
    "\n",
    "# Convert to minutes (Tableau-friendly)\n",
    "daily_hourly['total_ride_duration_min'] = daily_hourly['total_ride_duration_sec'] / 60\n",
    "\n",
    "\n",
    "# Save both\n",
    "combined.to_csv(\"citibike_2025_cleaned_full.csv\", index=False)\n",
    "daily_hourly.to_csv(\"citibike_2025_daily_hourly_stations.csv\", index=False)\n",
    "\n",
    "print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74e4c7b6-f935-4054-aeba-57a980181e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: 100%|██████████████████████████████████| 50/50 [03:47<00:00,  4.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rides: 43,545,978\n",
      "Finding TOP 100 stations...\n",
      "Top stations: 102 unique (100 start + 100 end)\n",
      "Filtered rides: 3,247,071 (10000% of total)\n",
      "Calculating distances...\n",
      "Creating hourly station flows...\n"
     ]
    }
   ],
   "source": [
    "# In this code, only the top 100 stations are selected. Since the full dataset generates a large number of rows that are not supported by the BI tool, this reduced dataset is used to prepare the Station Network Overview dashboard.\n",
    "\n",
    "import os, pandas as pd, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_DIR = \"/Users/saurabh/Documents/New Haven/Curriculam/GA/Citi Bikes/datasets/citibike_2025_raw\"\n",
    "COL_ORDER = [\"ride_id\",\"rideable_type\",\"started_at\",\"ended_at\",\"start_station_name\",\"start_station_id\",\"end_station_name\",\"end_station_id\",\"start_lat\",\"start_lng\",\"end_lat\",\"end_lng\",\"member_casual\"]\n",
    "\n",
    "# Load + clean\n",
    "all_csv_paths = [os.path.join(root,f) for root,dirs,files in os.walk(BASE_DIR) for f in files if f.endswith('.csv')]\n",
    "print(f\"Found {len(all_csv_paths)} files\")\n",
    "\n",
    "all_dfs = []\n",
    "for full_path in tqdm(all_csv_paths, desc=\"Loading\"):\n",
    "    df = pd.read_csv(full_path, low_memory=False)\n",
    "    df = df.reindex(columns=COL_ORDER)\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'], errors='coerce')\n",
    "    df['ended_at'] = pd.to_datetime(df['ended_at'], errors='coerce')\n",
    "    df['month_year'] = df['started_at'].dt.strftime('%Y-%m')   \n",
    "    df['hour_started_at'] = df['started_at'].dt.hour    \n",
    "    all_dfs.append(df)\n",
    "\n",
    "combined = pd.concat(all_dfs, ignore_index=True)\n",
    "combined = combined.dropna(subset=['started_at','ended_at','start_station_name','end_station_name'])\n",
    "combined['ride_duration_sec'] = (combined['ended_at']-combined['started_at']).dt.total_seconds()\n",
    "combined = combined[combined['ride_duration_sec']>0]\n",
    "\n",
    "print(f\"Raw rides: {combined.shape[0]:,}\")\n",
    "\n",
    "\n",
    "print(\"Finding TOP 100 stations...\")\n",
    "top_start = combined['start_station_name'].value_counts().head(100).index\n",
    "top_end = combined['end_station_name'].value_counts().head(100).index\n",
    "top_stations = list(set(top_start).union(set(top_end)))  # BOTH!\n",
    "\n",
    "print(f\"Top stations: {len(top_stations)} unique (100 start + 100 end)\")\n",
    "\n",
    "\n",
    "combined = combined[\n",
    "    combined['start_station_name'].isin(top_stations) & \n",
    "    combined['end_station_name'].isin(top_stations)\n",
    "]\n",
    "\n",
    "print(f\"Filtered rides: {combined.shape[0]:,} ({100*combined.shape[0]/combined.shape[0]:.0%} of total)\")\n",
    "\n",
    "# Distance (only on filtered data)\n",
    "print(\"Calculating distances...\")\n",
    "with np.errstate(invalid='ignore'):\n",
    "    combined['distance_miles'] = 3959 * np.arccos(\n",
    "        np.sin(np.radians(combined['start_lat'])) * np.sin(np.radians(combined['end_lat'])) +\n",
    "        np.cos(np.radians(combined['start_lat'])) * np.cos(np.radians(combined['end_lat'])) * \n",
    "        np.cos(np.radians(combined['start_lng']) - np.radians(combined['end_lng']))\n",
    "    )\n",
    "\n",
    "print(\"Creating hourly station flows...\")\n",
    "station_flows = combined.groupby(['month_year','hour_started_at','start_station_name','end_station_name','member_casual']).agg({\n",
    "    'ride_id':'count',\n",
    "    'ride_duration_sec':'sum',\n",
    "    'distance_miles':'sum'\n",
    "}).reset_index()\n",
    "\n",
    "station_flows.columns = ['month_year','hour_started_at','start_station_name','end_station_name','member_casual',\n",
    "                        'total_trips','total_trip_duration_sec','total_distance_miles']\n",
    "\n",
    "station_flows['total_trip_duration_min'] = station_flows['total_trip_duration_sec'] / 60\n",
    "\n",
    "station_flows = station_flows[['month_year', 'hour_started_at', 'start_station_name', 'end_station_name', \n",
    "                              'member_casual', 'total_trips', 'total_trip_duration_min', \n",
    "                              'total_distance_miles']]\n",
    "\n",
    "# Optimize types\n",
    "station_flows['hour_started_at'] = station_flows['hour_started_at'].astype('int8')\n",
    "\n",
    "# Save\n",
    "output_file = \"citibike_2025_station_flows_TOP100.csv\"\n",
    "station_flows.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf5b0357-e2ac-4f1c-9112-0f1bba289566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shape: (43545978, 6)\n",
      "Unique stations: (2352, 3)\n",
      "Saved citibike_2025_stations_lookup.csv with columns: ['station_name', 'lat', 'lng']\n"
     ]
    }
   ],
   "source": [
    "# This code creates a unique list of station names along with their latitude and longitude. The input file used here is a merged dataset created from all raw/sub files.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "FULL_PATH = \"/Users/saurabh/Documents/New Haven/Curriculam/GA/Citi Bikes/Python Code/citibike_2025_cleaned_full.csv\"\n",
    "\n",
    "# 2. Read ONLY the columns needed for stations\n",
    "use_cols = [\n",
    "    \"start_station_name\", \"start_lat\", \"start_lng\",\n",
    "    \"end_station_name\",   \"end_lat\",   \"end_lng\"\n",
    "]\n",
    "df = pd.read_csv(FULL_PATH, usecols=use_cols)\n",
    "\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "\n",
    "# 3. Build start-station table\n",
    "start_stations = df[[\"start_station_name\", \"start_lat\", \"start_lng\"]].dropna()\n",
    "start_stations = start_stations.rename(columns={\n",
    "    \"start_station_name\": \"station_name\",\n",
    "    \"start_lat\": \"lat\",\n",
    "    \"start_lng\": \"lng\"\n",
    "})\n",
    "\n",
    "# 4. Build end-station table\n",
    "end_stations = df[[\"end_station_name\", \"end_lat\", \"end_lng\"]].dropna()\n",
    "end_stations = end_stations.rename(columns={\n",
    "    \"end_station_name\": \"station_name\",\n",
    "    \"end_lat\": \"lat\",\n",
    "    \"end_lng\": \"lng\"\n",
    "})\n",
    "\n",
    "# 5. Combine and keep unique stations\n",
    "stations = pd.concat([start_stations, end_stations], ignore_index=True)\n",
    "\n",
    "# Optional: round coordinates a bit to help deduplicate tiny float differences\n",
    "stations[\"lat\"] = stations[\"lat\"].round(6)\n",
    "stations[\"lng\"] = stations[\"lng\"].round(6)\n",
    "\n",
    "stations = stations.drop_duplicates(subset=[\"station_name\", \"lat\", \"lng\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Unique stations:\", stations.shape)\n",
    "\n",
    "# 6. Save the lookup file\n",
    "stations.to_csv(\"citibike_2025_stations_lookup.csv\", index=False)\n",
    "print(\"Saved citibike_2025_stations_lookup.csv with columns:\", list(stations.columns))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
